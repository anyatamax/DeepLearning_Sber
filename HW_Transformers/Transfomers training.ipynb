{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a62d887-01aa-4980-9220-3bf28c077504",
   "metadata": {},
   "source": [
    "### Домашнее задание Transformers Training (50 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d7452-febe-4d25-a9c1-005fcc26b35b",
   "metadata": {},
   "source": [
    "В этом домашнем задании требуется обучить несколько Transformer-based моделей в задаче машинного перевода. Для обучения можно воспользоваться текущим проектом, так и реализовать свой пайплайн обучения. Если будете использовать проект, теги **TODO** проекта отмечают, какие компоненты надо реализовать.\n",
    "В ноутбуке нужно только отобразить результаты обучения и выводы. Архитектура модели(количетсво слоев, размерность и тд) остается на ваш выбор.\n",
    "\n",
    "Ваш код обучения нужно выложить на ваш github, в строке ниже дать ссылку на него. В первую очередь будут оцениваться результаты в ноутбуке, код нужен для проверки адекватности результатов. \n",
    "\n",
    "Обучать модели до конца не нужно, только для демонстрации, что модель обучается и рабочая - снижение val_loss, рост bleu_score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a691e7b9-6538-42b1-96b8-838f2efab4af",
   "metadata": {},
   "source": [
    "#### Сcылка на ваш github с проектом(вставить свой) - https://github.com/runnerup96/pytorch-machine-translation\n",
    "\n",
    "Ноутбук с результатами выкладывать на ваш **google диск** курса. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca1e62-b210-4426-9854-55b652417a4b",
   "metadata": {},
   "source": [
    "### Данные\n",
    "\n",
    "`\n",
    "wget https://www.manythings.org/anki/rus-eng.zip && unzip rus-eng.zip\n",
    "`\n",
    "\n",
    "Модели нужно обучить на задаче перевода с английского на русский. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "710e84de-f7a4-46da-a611-74c90a67e24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-29 11:01:45--  https://www.manythings.org/anki/rus-eng.zip\n",
      "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
      "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16305013 (16M) [application/zip]\n",
      "Saving to: ‘rus-eng.zip’\n",
      "\n",
      "rus-eng.zip         100%[===================>]  15,55M  4,92MB/s    in 3,2s    \n",
      "\n",
      "2024-04-29 11:01:49 (4,92 MB/s) - ‘rus-eng.zip’ saved [16305013/16305013]\n",
      "\n",
      "Archive:  rus-eng.zip\n",
      "  inflating: rus.txt                 \n",
      "  inflating: _about.txt              \n"
     ]
    }
   ],
   "source": [
    "# !wget https://www.manythings.org/anki/rus-eng.zip && unzip rus-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "MATCH_REGEX = re.compile(r\"[+-]?\\b(\\d+([.]\\d*)?([eE][+-]?\\d+)?|[.]\\d+([eE][+-]?\\d+)?)\\b\")\n",
    "\n",
    "def extract_values(string):\n",
    "    result = dict()\n",
    "    params = ['val_loss', 'train_loss', 'bleu_score']\n",
    "    if \"train_loss\" in string:\n",
    "        found_vals = [re_match[0] for re_match in re.findall(MATCH_REGEX, string)]\n",
    "    \n",
    "        if len(params) == len(found_vals):\n",
    "            for name, val in zip(params, found_vals):\n",
    "                result[name] = float(val)\n",
    "    return result\n",
    "\n",
    "def plot_results(train_loss_list, val_loss_list, val_bleu_list, run_name):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5))\n",
    "    \n",
    "    ax1.plot(range(len(train_loss_list)), train_loss_list, label='train loss')\n",
    "    ax1.plot(range(len(val_loss_list)), val_loss_list, label='val loss')\n",
    "    ax1.set_xlabel('epoch')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.plot(range(len(val_bleu_list)), val_bleu_list, label='val bleu')\n",
    "    ax2.set_xlabel('epoch')\n",
    "    ax2.set_ylabel('BLEU')\n",
    "    ax2.legend()\n",
    "    \n",
    "    fig.suptitle(run_name, fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_random_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(1, \"./src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc772a6e-7d1a-4d8d-8024-0454a948835b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Обучение Seq2seq Transformer модель(25 баллов)\n",
    "\n",
    "Реализуйте Seq2seq Transformer. В качестве блока трансформера можно использовать https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html. В качестве токенизатора воспользуйтесь HuggingFace токенизатор для source/target языков - https://huggingface.co/docs/transformers/fast_tokenizers\n",
    "В качестве максимальной длинны возьмите предложения длинной **до 15 слов**, без каких либо префиксов. \n",
    "\n",
    "Не забудьте остальные элементы модели:\n",
    "* Мы можем использовать 1 трансформер как энкодер - декодером будет выступать линейный слой. \n",
    "* Обучите свой BPE токенизатор - https://huggingface.co/docs/transformers/fast_tokenizers\n",
    "* Матрицу эмбеддингов токенов\n",
    "* Матрицу позицонных эмбеддингов\n",
    "* Линейный слой проекции в target словарь\n",
    "* Функцию маскирования будущих состояний attention, так как модель авто-регрессионна\n",
    "* Learning rate schedualer\n",
    "\n",
    "\n",
    "В качестве результатов, приложите слудующие данные:\n",
    "1) Параметры обучения - learning rate, batch_size, epoch_num, размерность скрытого слоя, количетсво слоев\n",
    "2) Графики обучения - train loss, val loss, bleu score\n",
    "3) Примеры переводов вашей модели(10 штук) - source text, true target text, predicted target text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from models import trainer\n",
    "from data.datamodule import DataManager\n",
    "from txt_logger import TXTLogger\n",
    "from models.seq2seq_transformer import Seq2SeqTransformer\n",
    "from models.seq2seq_t5 import Seq2SeqT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a7ce070-1e45-47ac-a07a-c36b6baa114e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prefix_filter': [],\n",
       " 'max_length': 15,\n",
       " 'filename': 'rus.txt',\n",
       " 'train_size': 0.8,\n",
       " 'batch_size': 256}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_config = yaml.load(open(\"configs/data_config.yaml\", 'r'), Loader=yaml.Loader)\n",
    "data_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading from file: 100%|██████████| 496059/496059 [00:02<00:00, 206922.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_manager = DataManager(data_config, DEVICE)\n",
    "train_dataloader, val_dataloader = data_manager.prepare_data()\n",
    "target_tokenizer = data_manager.target_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embedding_size': 256,\n",
       " 'dim_feedforward': 256,\n",
       " 'hidden_size': 256,\n",
       " 'n_heads_attention': 4,\n",
       " 'n_encoders': 6,\n",
       " 'n_decoders': 6,\n",
       " 'learning_rate': 0.0001,\n",
       " 'dropout': 0.1,\n",
       " 'epoch_num': 10,\n",
       " 'try_one_batch': False}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = yaml.load(open(\"configs/model_config.yaml\", \"r\"), Loader=yaml.Loader)\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anymax/Documents/mipt/DL_Sber/venv_dl/lib/python3.12/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model_transformer = Seq2SeqTransformer(\n",
    "    encoder_vocab_size=len(data_manager.source_tokenizer.word2index),\n",
    "    decoder_vocab_size=len(data_manager.target_tokenizer.word2index),\n",
    "    dim_feedforward=model_config['dim_feedforward'],\n",
    "    lr=model_config['learning_rate'],\n",
    "    device=DEVICE,\n",
    "    target_tokenizer=target_tokenizer,\n",
    "    T=model_config['epoch_num'] * len(train_dataloader),\n",
    "    positional_embedding_size=model_config['embedding_size'],\n",
    "    n_heads_attention=model_config['n_heads_attention'],\n",
    "    n_encoders=model_config['n_encoders'],\n",
    "    n_decoders=model_config['n_decoders'],\n",
    "    dropout=model_config['dropout'],\n",
    ")\n",
    "\n",
    "logger = TXTLogger(\"training_logs_transformer\")\n",
    "trainer_transformer = trainer.Trainer(model=model_transformer, model_config=model_config, logger=logger)\n",
    "\n",
    "if model_config['try_one_batch']:\n",
    "    train_dataloader = [list(train_dataloader)[0]]\n",
    "    val_dataloader = [list(val_dataloader)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0338a9dbe7e4583bf4ef22daccc10b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903a2ff697924039a87a1f0c2cff10ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1005 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_transformer.train(train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245297ae-d62b-4227-9312-ccff5a1c13c9",
   "metadata": {},
   "source": [
    "### Fine-tune pretrained T5 (25 баллов)\n",
    "\n",
    "Реализуйте Seq2seq Pretrained T5. Воспользуйтесь https://huggingface.co/docs/transformers/model_doc/t5 предобученной моделью. В качестве максимальной длинны возьмите предложения длинной **до 15 слов**, без каких либо префиксов. Архитектура модели(количетсво слоев, размерность и тд) остается на ваш выбор.\n",
    "\n",
    "Не забудьте важные аспекты обучения модели:\n",
    "* Взять готовый t5 токенизатор\n",
    "* Resize matrix embedding - скорей всего ваша матрица эмбеддингов не будет включать эмбеддинги из вашего сета. Пример обновления матрицы эмбеддингов тут тут https://github.com/runnerup96/Transformers-Tuning/blob/main/t5_encoder_decoder.py\n",
    "* Learning rate schedualer/Adafactor with constant learning rate\n",
    "\n",
    "\n",
    "В качестве результатов, приложите слудующие данные:\n",
    "1) Параметры обучения - learning rate, batch_size, epoch_num, pretrained model name\n",
    "2) Графики обучения - train loss, val loss, bleu score\n",
    "3) Примеры переводов вашей модели(10 штук) - source text, true target text, predicted target text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5993bdc9-44f6-4af8-8e46-36d4e6824ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
