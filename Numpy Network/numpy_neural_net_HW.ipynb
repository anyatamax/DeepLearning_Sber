{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8709923-3976-4762-87b0-88701ba0f41a",
   "metadata": {},
   "source": [
    "## Домашнее задание Numpy Neural Net Exploration (50 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e04e1f-f5fb-473a-8b22-dca1b287c557",
   "metadata": {},
   "source": [
    "В этом домашнем задании требуется провести набор экспериментов с нейронной сетью. Для проведения экспериментов нужно доработать представленный фреймворк для обучения нейронной сети на Numpy в проекте **Numpy_NN** c **hinge_loss** функцией ошибки. \n",
    "\n",
    "\n",
    "Все необходимые доработки обозначены в проекте `Numpy_NN` тегом **TODO**.\n",
    "\n",
    "В ноутбуке только сами эксперименты с помощью данных команд - все доработки делать внутри фреймворка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d42d7fc1-7a72-4c70-922a-b8b5f8c9e907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'nn.module.sequential' from '/Users/anymax/Documents/mipt/DL_Sber/DL_Sber/HW_1/Numpy Network/Numpy_NN/src/nn/module/sequential.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пример импорта модуля\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), \"Numpy_NN/src\"))\n",
    "\n",
    "from nn.module import sequential\n",
    "\n",
    "# пример быстрой перезагрузки модуля без рестарта ноутбука\n",
    "import importlib\n",
    "importlib.reload(sequential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d38b0b5-39c2-43fc-a69d-e8ed8ad0b02b",
   "metadata": {},
   "source": [
    "Вам нужно провести набор экспериментов с CIFAR датасетом и исследовать поведение нейронной сети на этом датасете при разных архитектурах. Пример 1ого эксперимента можно найти в ноутбуке ``Numpy Network/Numpy_NN/test_CIFAR_training.ipynb``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b802c7a0-96c9-4ecd-a8ac-05e2ab153917",
   "metadata": {},
   "source": [
    "Ваши эксперименты должы быть логичны - старайтесь последовательно строить архитектуру и попытайтесь к концу эксприментов определить комбинацию компонент нейронной сети для лучшего качества. Также же вы можете расширять количеcтво экспериментов, подтверждая это соответсвующими выводами. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f13db88-c031-41ed-a828-b3382cb25b6a",
   "metadata": {},
   "source": [
    "В каждом эксперименте должен быть:\n",
    "    \n",
    "    * Процесс обучения нейронной сети с помощью training.train.train\n",
    "    * Проверка градиента для каждой архитектуры нейронной сети с помощью utils.gradient_check(если это указано в описании эксперимента)\n",
    "    * Визуализация обучения - воспользоваться методом visualization_utils.plot_learning_curves\n",
    "    * Рассчет точности общей точности модели и рассчет по классовой точности с помощью sklearn.metrics.classification_report\n",
    "    * Ваш вывод по результату эксперимента\n",
    "    \n",
    "В каждом эксперименте должен быть каждый пункт иначе эксперимент не оценивается. Каждый эксперимент оценивается в **4 балла**. За расширение пространства экспериментов и реализации своих идей можно дополнительно получить до **6 баллов**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0783f5fb-28e6-4632-8132-f10d98d909a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9900f3f7-2605-4513-beb6-c82783ea9658",
   "metadata": {},
   "source": [
    "В данном ДЗ оценивается качество работы доработанного вами фреймворка, качество проведения эксперимента и полученный результат. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a39c43",
   "metadata": {},
   "source": [
    "### Для начала проанализируем датасет и разделим на три части\n",
    "Этот ход скопирован из примера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6cd1a0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-21 23:15:26--  https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
      "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 170498071 (163M) [application/x-gzip]\n",
      "Saving to: ‘cifar-10-python.tar.gz’\n",
      "\n",
      "cifar-10-python.tar 100%[===================>] 162,60M  1022KB/s    in 2m 21s  \n",
      "\n",
      "2024-02-21 23:17:47 (1,15 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb09566",
   "metadata": {},
   "source": [
    "И распакуем его"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b15640c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzf cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec39b64d",
   "metadata": {},
   "source": [
    "На выходе будем иметь следующие файлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dca18b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-batches.meta\n",
      "-data_batch_1\n",
      "-data_batch_2\n",
      "-data_batch_3\n",
      "-data_batch_4\n",
      "-data_batch_5\n",
      "-readme.html\n",
      "-test_batch\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob(\"cifar-10-batches-py/**\")\n",
    "files = [item.split('/')[-1] for item in files]\n",
    "for file in sorted(files):\n",
    "    print(f\"-{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8521f53",
   "metadata": {},
   "source": [
    "Из этих файлов нас интересуют data_batch_1, data_batch_2, ..., data_batch_5 -- это файлы с тренировочными данными, и файл test_batch -- это файл с тестовыми данными, на которых мы будем измерять свои метрики.\n",
    "\n",
    "Посмотрим ближе на структуру файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6df77f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cifar-10-batches-py/data_batch_1\", \"rb\") as f:\n",
    "    train_dataset_1 = pickle.load(f, encoding=\"bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af23efb1",
   "metadata": {},
   "source": [
    "Датасет содержит следующие ключи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c859dcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-batch_label\n",
      "-labels\n",
      "-data\n",
      "-filenames\n"
     ]
    }
   ],
   "source": [
    "for key in train_dataset_1.keys():\n",
    "    print(f\"-{key.decode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adb440d",
   "metadata": {},
   "source": [
    "Ключ `batch_label` нас не интересует, так как содержит только лишь информацию о номере бача"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88902730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training batch 1 of 5'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_1[b'batch_label'].decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a416fc6b",
   "metadata": {},
   "source": [
    "Ключ `filenames` в данном случае нас тоже не интересует, так как он содержит названия файлов, которые мы без привлечения языковых моделей использовать не сможем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f383675c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые 10 названий:\n",
      "-leptodactylus_pentadactylus_s_000004.png\n",
      "-camion_s_000148.png\n",
      "-tipper_truck_s_001250.png\n",
      "-american_elk_s_001521.png\n",
      "-station_wagon_s_000293.png\n",
      "-coupe_s_001735.png\n",
      "-cassowary_s_001300.png\n",
      "-cow_pony_s_001168.png\n",
      "-sea_boat_s_001584.png\n",
      "-tabby_s_001355.png\n"
     ]
    }
   ],
   "source": [
    "print(\"Первые 10 названий:\")\n",
    "for name in train_dataset_1[b'filenames'][:10]:\n",
    "    print(f\"-{name.decode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10153b8a",
   "metadata": {},
   "source": [
    "Остаются два наиболее интересных ключа, это `data` и `labels`.\n",
    "\n",
    "Как нетрудно догадаться, `labels` содержит классы. Выведем первые 10 элементов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa618799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 9, 9, 4, 1, 1, 2, 7, 8, 3]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_1[b'labels'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dd0f6d",
   "metadata": {},
   "source": [
    "Классы представлены числовыми метками. В принципе нам нет необходимости знать соответствие между ними, однако эти данные можно найти в файле `batches.meta`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10632184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: airplane\n",
      "1: automobile\n",
      "2: bird\n",
      "3: cat\n",
      "4: deer\n",
      "5: dog\n",
      "6: frog\n",
      "7: horse\n",
      "8: ship\n",
      "9: truck\n"
     ]
    }
   ],
   "source": [
    "with open(\"cifar-10-batches-py/batches.meta\", \"rb\") as f:\n",
    "    classes_names = pickle.load(f, encoding=\"bytes\")\n",
    "\n",
    "for ind, name in enumerate(classes_names[b'label_names']):\n",
    "    print(f\"{ind}: {name.decode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d02389",
   "metadata": {},
   "source": [
    "Ключ `data` содержит уже сами числовые данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f69a4ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество данных: 10000\n",
      "Размер вектора: (3072,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Количество данных: {len(train_dataset_1[b'data'])}\")\n",
    "print(f\"Размер вектора: {train_dataset_1[b'data'][0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb23904",
   "metadata": {},
   "source": [
    "Теперь интересно посмотреть на сбалансированность меток по разбиениям. Для этого загрузим все тестовые разбиения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4079aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cifar-10-batches-py/data_batch_2\", \"rb\") as f:\n",
    "    train_dataset_2 = pickle.load(f, encoding=\"bytes\")\n",
    "\n",
    "with open(\"cifar-10-batches-py/data_batch_3\", \"rb\") as f:\n",
    "    train_dataset_3 = pickle.load(f, encoding=\"bytes\")\n",
    "\n",
    "with open(\"cifar-10-batches-py/data_batch_4\", \"rb\") as f:\n",
    "    train_dataset_4 = pickle.load(f, encoding=\"bytes\")\n",
    "\n",
    "with open(\"cifar-10-batches-py/data_batch_5\", \"rb\") as f:\n",
    "    train_dataset_5 = pickle.load(f, encoding=\"bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39a31bc",
   "metadata": {},
   "source": [
    "И посчитаем сколько раз каждая метка встречается в каждом датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5921a97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|=====|======|======|======|======|======|======|\n",
      "|Label|Data 1|Data 2|Data 3|Data 4|Data 5| Sum  |\n",
      "|=====|======|======|======|======|======|======|\n",
      "|  0  | 1005 | 984  | 994  | 1003 | 1014 | 5000 |\n",
      "|  1  | 974  | 1007 | 1042 | 963  | 1014 | 5000 |\n",
      "|  2  | 1032 | 1010 | 965  | 1041 | 952  | 5000 |\n",
      "|  3  | 1016 | 995  | 997  | 976  | 1016 | 5000 |\n",
      "|  4  | 999  | 1010 | 990  | 1004 | 997  | 5000 |\n",
      "|  5  | 937  | 988  | 1029 | 1021 | 1025 | 5000 |\n",
      "|  6  | 1030 | 1008 | 978  | 1004 | 980  | 5000 |\n",
      "|  7  | 1001 | 1026 | 1015 | 981  | 977  | 5000 |\n",
      "|  8  | 1025 | 987  | 961  | 1024 | 1003 | 5000 |\n",
      "|  9  | 981  | 985  | 1029 | 983  | 1022 | 5000 |\n",
      "|=====|======|======|======|======|======|======|\n"
     ]
    }
   ],
   "source": [
    "datasets = [train_dataset_1, train_dataset_2, train_dataset_3, \n",
    "            train_dataset_4, train_dataset_5]\n",
    "\n",
    "labels = list(range(10))\n",
    "\n",
    "print(\"|=====|{}|{}|{}|{}|{}|{}|\".format(\n",
    "    \"=\" * 6, \"=\" * 6, \"=\" * 6, \"=\" * 6, \"=\" * 6, \"=\" * 6))\n",
    "print(\"|Label|{0:^{1}}|{2:^{3}}|{4:^{5}}|{6:^{7}}|{8:^{9}}|{10:^{11}}|\".format(\n",
    "    \"Data 1\", 6, \"Data 2\", 6, \"Data 3\", 6, \"Data 4\", 6, \"Data 5\", 6, \"Sum\", 6))\n",
    "print(\"|=====|{}|{}|{}|{}|{}|{}|\".format(\n",
    "    \"=\" * 6, \"=\" * 6, \"=\" * 6, \"=\" * 6, \"=\" * 6, \"=\" * 6))\n",
    "\n",
    "for label in labels:\n",
    "    print(\"|{0:^{1}}|\".format(label, 5), end='')\n",
    "    label_num = 0\n",
    "    for dataset in datasets:\n",
    "        print(\"{0:^{1}}|\".format(dataset[b'labels'].count(label), 6), end='')\n",
    "        label_num += dataset[b'labels'].count(label)\n",
    "    print(\"{0:^{1}}|\".format(label_num, 6))\n",
    "\n",
    "print(\"|=====|{}|{}|{}|{}|{}|{}|\".format(\n",
    "    \"=\" * 6, \"=\" * 6, \"=\" * 6, \"=\" * 6, \"=\" * 6, \"=\" * 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e76a616",
   "metadata": {},
   "source": [
    "Как видим, данные не сбалансированы по разбиениям, поэтому объединим все в один датасет и разобьем это на два датасета, тренировочный и валидационный, при этом сразу поделим значения каждого вектора на 256, чтобы иметь интенсивность в относительных единицах (хотя максимум на 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c31ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    for vec, label in zip(dataset[b'data'], dataset[b'labels']):\n",
    "        train_dataset.append((vec / 256, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f14c1be",
   "metadata": {},
   "source": [
    "На валидационную выборку оставим 10000 элементов, по 1000 примеров из каждого класса, выбранных случайно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86a984aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "np.random.shuffle(train_dataset)\n",
    "\n",
    "train_data = []\n",
    "valid_data = []\n",
    "\n",
    "cnt = {ind: 0 for ind in range(10)}\n",
    "\n",
    "for vec, label in train_dataset:\n",
    "    if cnt[label] < 1000:\n",
    "        cnt[label] += 1\n",
    "        valid_data.append((vec, label))\n",
    "    else:\n",
    "        train_data.append((vec, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a8ab5f",
   "metadata": {},
   "source": [
    "Подготовим еще тестовый датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f661054",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cifar-10-batches-py/test_batch\", \"rb\") as f:\n",
    "    test_dataset = pickle.load(f, encoding=\"bytes\")\n",
    "\n",
    "test_data = []\n",
    "for vec, label in zip(test_dataset[b'data'], test_dataset[b'labels']):\n",
    "    test_data.append((vec / 256, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ed6a21",
   "metadata": {},
   "source": [
    "Посмотрим на статистики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efeac51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|=====|======|======|======|======|\n",
      "|Label|Train |Valid | Test | Sum  |\n",
      "|=====|======|======|======|======|\n",
      "|  0  | 4000 | 1000 | 1000 | 6000 |\n",
      "|  1  | 4000 | 1000 | 1000 | 6000 |\n",
      "|  2  | 4000 | 1000 | 1000 | 6000 |\n",
      "|  3  | 4000 | 1000 | 1000 | 6000 |\n",
      "|  4  | 4000 | 1000 | 1000 | 6000 |\n",
      "|  5  | 4000 | 1000 | 1000 | 6000 |\n",
      "|  6  | 4000 | 1000 | 1000 | 6000 |\n",
      "|  7  | 4000 | 1000 | 1000 | 6000 |\n",
      "|  8  | 4000 | 1000 | 1000 | 6000 |\n",
      "|  9  | 4000 | 1000 | 1000 | 6000 |\n",
      "|=====|======|======|======|======|\n"
     ]
    }
   ],
   "source": [
    "print(\"|=====|{}|{}|{}|{}|\".format(\"=\" * 6, \"=\" * 6, \"=\" * 6, \"=\" * 6))\n",
    "print(\"|Label|{0:^{1}}|{2:^{3}}|{4:^{5}}|{6:^{7}}|\".format(\n",
    "    \"Train\", 6, \"Valid\", 6, \"Test\", 6, \"Sum\", 6))\n",
    "print(\"|=====|{}|{}|{}|{}|\".format(\"=\" * 6, \"=\" * 6, \"=\" * 6, \"=\" * 6))\n",
    "\n",
    "labels = list(range(10))\n",
    "for label in labels:\n",
    "    print(\"|{0:^{1}}|\".format(label, 5), end='')\n",
    "    label_num = 0\n",
    "    for dataset in (train_data, valid_data, test_data):\n",
    "        labels_data = [item[1] for item in dataset]\n",
    "        print(\"{0:^{1}}|\".format(labels_data.count(label), 6), end='')\n",
    "        label_num += labels_data.count(label)\n",
    "    print(\"{0:^{1}}|\".format(label_num, 6))\n",
    "\n",
    "print(\"|=====|{}|{}|{}|{}|\".format(\"=\" * 6, \"=\" * 6, \"=\" * 6, \"=\" * 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c740748b",
   "metadata": {},
   "source": [
    "Перед обучением отнормируем все данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64931bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = np.array([item[0] for item in train_data])\n",
    "D = np.var(train_matrix, axis=0)\n",
    "E = np.mean(train_matrix, axis=0)\n",
    "\n",
    "for ind in range(len(train_data)):\n",
    "    train_data[ind] = (train_data[ind][0] - E, train_data[ind][1])\n",
    "    train_data[ind] = (train_data[ind][0] / np.sqrt(D), train_data[ind][1])\n",
    "\n",
    "for ind in range(len(valid_data)):\n",
    "    valid_data[ind] = (valid_data[ind][0] - E, valid_data[ind][1])\n",
    "    valid_data[ind] = (valid_data[ind][0] / np.sqrt(D), valid_data[ind][1])\n",
    "\n",
    "for ind in range(len(test_data)):\n",
    "    test_data[ind] = (test_data[ind][0] - E, test_data[ind][1])\n",
    "    test_data[ind] = (test_data[ind][0] / np.sqrt(D), test_data[ind][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da100e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.dataloader import Dataloader\n",
    "from nn.module.sequential import Sequential\n",
    "from nn.layers.linear import Linear\n",
    "from nn.layers.batch_norm import BatchNorm\n",
    "from nn.layers.dropout import Dropout\n",
    "from nn.activations.relu import ReLU\n",
    "from nn.activations.sigmoid import Sigmoid\n",
    "from nn.activations.tanh import Tanh\n",
    "from optimization.adam_optimizer import Adam\n",
    "from nn import loss_functions\n",
    "from training.train import train\n",
    "from nn.loss_functions.hinge_loss import hinge_loss\n",
    "from utils import gradient_check\n",
    "import visualization_utils as viz_utils\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00045a7e-cef6-47ce-8579-b0d442c56873",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Эксперимент 1 Построение нейронной сети с ReLU функцией активации\n",
    "\n",
    "Реализуйте эксперимент, доработав соответсвующие модули Numpy_NN фреймворка. Количество слоев, функций на ваше усмотрение. Можно добавлять любые другие функции активации на ваше усмотрение, но указанная в эксперименте функция должна быть обязательно. Перед обучением проверьте с помощью utils.gradient_check градиент вашей модели.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87c4494",
   "metadata": {},
   "source": [
    "Построим простую трехслойную модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29d09ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "164dda97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "\tLinear(3072, 128, bias=True),\n",
       "\tReLU(),\n",
       "\tLinear(128, 20, bias=True),\n",
       "\tReLU(),\n",
       "\tLinear(20, 10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверка градиентов:\n",
    "check_model = Sequential(\n",
    "    Linear(3072, 128),\n",
    "    ReLU(),\n",
    "    Linear(128, 20),\n",
    "    ReLU(),\n",
    "    Linear(20, 10)\n",
    ")\n",
    "check_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3d4e2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверка пройдена                                                                           \n"
     ]
    }
   ],
   "source": [
    "x, y = Dataloader(train_data, batch_size=2).__next__()\n",
    "print(f\"Проверка{' не пройдена' if not gradient_check(x, y, check_model) else ''} пройдена\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ea3272-aa90-4e7d-acc9-07649dc529c1",
   "metadata": {},
   "source": [
    "### Эксперимент 2 Построение нейронной сети с Sigmoid функцией активации\n",
    "\n",
    "Реализуйте эксперимент, доработав соответсвующие модули Numpy_NN фреймворка. Количество слоев, функций на ваше усмотрение. Можно добавлять любые другие функции активации на ваше усмотрение, но указанная в эксперименте функция должна быть обязательно. Перед обучением проверьте с помощью utils.gradient_check градиент вашей модели.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05caad82-f228-4a99-8524-b10af6feaa83",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Эксперимент 3 Построение нейронной сети с TanH функцией активации\n",
    "\n",
    "Реализуйте эксперимент, доработав соответсвующие модули Numpy_NN фреймворка. Количество слоев, функций на ваше усмотрение. Можно добавлять любые другие функции активации на ваше усмотрение, но указанная в эксперименте функция должна быть обязательно. Перед обучением проверьте с помощью utils.gradient_check градиент вашей модели.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24afa6f2-c995-4ac5-9abf-e67325dd1b58",
   "metadata": {},
   "source": [
    "### Эксперимент 4 Построение нейронной сети с Dropout слоем\n",
    "\n",
    "Реализуйте эксперимент, доработав соответсвующие модули Numpy_NN фреймворка. Количество слоев, функций на ваше усмотрение. Можно добавлять любые другие функции активации, слои на ваше усмотрение, но указанный в эксперименте слой должен быть обязательно. Перед обучением проверьте с помощью utils.gradient_check градиент вашей модели.  Подберите оптимальную вероятность в dropout. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c6f53d-1705-4914-990f-105cd89642aa",
   "metadata": {},
   "source": [
    "### Эксперимент 5 Построение нейронной сети с Batchnorm слоем\n",
    "\n",
    "Реализуйте эксперимент, доработав соответсвующие модули Numpy_NN фреймворка. Количество слоев, функций на ваше усмотрение. Можно добавлять любые другие функции активации, слои на ваше усмотрение, но указанный в эксперименте слой должен быть обязательно. Перед обучением проверьте с помощью utils.gradient_check градиент вашей модели.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a511afea-159a-45a9-a7bd-aa453f619a53",
   "metadata": {},
   "source": [
    "### Эксперимент 6 Эксперимент с Adam оптимизатором \n",
    "\n",
    "Обучите вашу нейронную сеть с Adam оптимизатором. Подбор гиперпараметров на ваше усмотрение. Перед обучением проверьте с помощью utils.gradient_check градиент вашей модели.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0f04d0-df36-402c-8fe4-88558d5d4e61",
   "metadata": {},
   "source": [
    "### Эксперимент 7 Эксперимент c количеством слоев\n",
    "\n",
    "Зафиксируйте модель и начинайте добавлять слои. Как меняется качество модели?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a038d1e5-af04-4236-a4bc-ea6350979d3f",
   "metadata": {},
   "source": [
    "### Эксперимент 8 Эксперимент c количеством нейронов\n",
    "\n",
    "Зафиксируйте модель и увеличьте количетсво весов в одном из слоев. Дает ли это прирост качества?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13eba62-4994-4385-923d-ba5c76195ddd",
   "metadata": {},
   "source": [
    "### Эксперимент 9 Эксперимент c размером батча\n",
    "\n",
    "Зафиксируйте модель и обучитесь на разных размерах. Как размер батча влияет на процесс обучения?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1d0740-2fad-4b58-9025-873d7caa0667",
   "metadata": {},
   "source": [
    "### Эксперимент 10 Эксперимент с регуляризацией\n",
    "\n",
    "Добавьте в процесс обучения L1/L2 регуляризацию. Повлияло ли это на качесвто? Выведите норму весов без регуляризации и с. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee79fe-17b4-4363-a796-7880d2aead84",
   "metadata": {},
   "source": [
    "### Эксперимент 11 Финальная архитектура\n",
    "\n",
    "Как выглядит ваша финальная модель?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
